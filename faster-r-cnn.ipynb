{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12017419,"sourceType":"datasetVersion","datasetId":7560653}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install xmltodict\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-31T14:51:49.286122Z","iopub.execute_input":"2025-05-31T14:51:49.286398Z","iopub.status.idle":"2025-05-31T14:51:52.238996Z","shell.execute_reply.started":"2025-05-31T14:51:49.286364Z","shell.execute_reply":"2025-05-31T14:51:52.238005Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport xmltodict\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport torchvision.transforms as T\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport numpy as np\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T14:51:52.240991Z","iopub.execute_input":"2025-05-31T14:51:52.241404Z","iopub.status.idle":"2025-05-31T14:51:55.834074Z","shell.execute_reply.started":"2025-05-31T14:51:52.241380Z","shell.execute_reply":"2025-05-31T14:51:55.833471Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class VOCDataset(Dataset):\n    def __init__(self, image_dir, annotation_dir, classes, transforms=None):\n        self.image_dir = image_dir\n        self.annotation_dir = annotation_dir\n        self.transforms = transforms\n        self.classes = classes\n        self.image_files = [f for f in os.listdir(image_dir) if f.endswith(\".jpg\")]\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.image_dir, self.image_files[idx])\n        ann_path = os.path.join(self.annotation_dir, self.image_files[idx].replace(\".jpg\", \".xml\"))\n        img = Image.open(img_path).convert(\"RGB\")\n\n        with open(ann_path) as f:\n            ann = xmltodict.parse(f.read())[\"annotation\"]\n\n        boxes, labels = [], []\n        objs = ann[\"object\"]\n        if not isinstance(objs, list):\n            objs = [objs]\n\n        for obj in objs:\n            label = self.classes.index(obj[\"name\"])\n            bbox = obj[\"bndbox\"]\n\n            xmin = float(bbox[\"xmin\"])\n            ymin = float(bbox[\"ymin\"])\n            xmax = float(bbox[\"xmax\"])\n            ymax = float(bbox[\"ymax\"])\n\n            # ‚úÖ B·ªè qua bbox kh√¥ng h·ª£p l·ªá (zero ho·∫∑c √¢m width/height)\n            if xmax <= xmin or ymax <= ymin:\n                print(f\"‚ö†Ô∏è B·ªè bbox l·ªói: {[xmin, ymin, xmax, ymax]} ·ªü ·∫£nh {self.image_files[idx]}\")\n                continue\n\n            boxes.append([xmin, ymin, xmax, ymax])\n            labels.append(label)\n\n        boxes = torch.tensor(boxes, dtype=torch.float32)\n        labels = torch.tensor(labels, dtype=torch.int64)\n        target = {\"boxes\": boxes, \"labels\": labels, \"image_id\": torch.tensor([idx])}\n\n        if self.transforms:\n            img = self.transforms(img)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.image_files)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T14:51:55.834692Z","iopub.execute_input":"2025-05-31T14:51:55.834955Z","iopub.status.idle":"2025-05-31T14:51:55.843598Z","shell.execute_reply.started":"2025-05-31T14:51:55.834939Z","shell.execute_reply":"2025-05-31T14:51:55.842683Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_fasterrcnn_model(num_classes):\n    model = fasterrcnn_resnet50_fpn(pretrained=True)\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T14:51:55.845296Z","iopub.execute_input":"2025-05-31T14:51:55.845541Z","iopub.status.idle":"2025-05-31T14:51:55.861148Z","shell.execute_reply.started":"2025-05-31T14:51:55.845523Z","shell.execute_reply":"2025-05-31T14:51:55.860472Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nclasses = [\"MR\", \"NC\", \"WF\"]\ndata_dir = \"/kaggle/input/yellow-sticky-traps-dataset\" # s·ª≠a th√†nh t√™n dataset c·ªßa b·∫°n\n\ntransforms = T.Compose([T.ToTensor()])\ntrainset = VOCDataset(f\"{data_dir}/train\", f\"{data_dir}/train\", classes, transforms)\nvalidset = VOCDataset(f\"{data_dir}/valid\", f\"{data_dir}/valid\", classes, transforms)\n\n\ntrainloader = DataLoader(trainset, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\nvalidloader = DataLoader(validset, batch_size=1, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n\nmodel = get_fasterrcnn_model(num_classes=len(classes) + 1)\nmodel.to(device)\n\noptimizer = torch.optim.SGD([p for p in model.parameters() if p.requires_grad],\n                            lr=0.005, momentum=0.9)\n\nnum_epochs = 50\nloss_history = []\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0.0\n    for images, targets in trainloader:\n        images = [img.to(device) for img in images]\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n        total_loss += losses.item()\n\n    avg_loss = total_loss / len(trainloader)\n    loss_history.append(avg_loss)\n    print(f\"Epoch {epoch+1} - Loss: {avg_loss:.4f}\")\n\ntorch.save(model.state_dict(), \"fasterrcnn_model.pth\")\nprint(\"‚úÖ M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c l∆∞u.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T14:51:55.861885Z","iopub.execute_input":"2025-05-31T14:51:55.862061Z","iopub.status.idle":"2025-05-31T15:50:29.285403Z","shell.execute_reply.started":"2025-05-31T14:51:55.862047Z","shell.execute_reply":"2025-05-31T15:50:29.284565Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(loss_history, marker='o', label=\"Loss\")\nplt.title(\"Training Loss over Epochs\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.grid(True)\nplt.legend()\nplt.savefig(\"loss_curve.png\")\nplt.show()\n\n# ‚úÖ T·∫°m th·ªùi t√≠nh accuracy l√† s·ªë l∆∞·ª£ng d·ª± ƒëo√°n ƒë√∫ng label ƒë·∫ßu ti√™n tr√πng v·ªõi label th·∫≠t (g·∫ßn ƒë√∫ng v·ªõi object detection)\ncorrect = 0\ntotal = 0\nmodel.eval()\n\nwith torch.no_grad():\n    for images, targets in validloader:\n        images = [img.to(device) for img in images]\n        outputs = model(images)\n\n        for i in range(len(outputs)):\n            pred_labels = outputs[i]['labels'].cpu().numpy()\n            true_labels = targets[i]['labels'].cpu().numpy()\n\n            matched = sum(p == t for p, t in zip(pred_labels[:len(true_labels)], true_labels))\n            correct += matched\n            total += len(true_labels)\n\naccuracy = 100 * correct / total if total > 0 else 0\nprint(f\"‚úÖ Approximate Accuracy: {accuracy:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T15:50:29.286506Z","iopub.execute_input":"2025-05-31T15:50:29.286811Z","iopub.status.idle":"2025-05-31T15:50:33.198674Z","shell.execute_reply.started":"2025-05-31T15:50:29.286784Z","shell.execute_reply":"2025-05-31T15:50:33.198003Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\n\nmodel.eval()\ny_true = []\ny_pred = []\n\nwith torch.no_grad():\n    for images, targets in validloader:\n        images = [img.to(device) for img in images]\n        outputs = model(images)\n\n        for i in range(len(outputs)):\n            pred_labels = outputs[i]['labels'].cpu().numpy()\n            true_labels = targets[i]['labels'].cpu().numpy()\n\n            # L·∫•y s·ªë bbox b·∫±ng nhau (tr√°nh mismatch)\n            min_len = min(len(pred_labels), len(true_labels))\n            y_true.extend(true_labels[:min_len])\n            y_pred.extend(pred_labels[:min_len])\n\nprint(\"üìã Classification Report:\")\nprint(classification_report(y_true, y_pred, target_names=classes))\n\nprint(\"üìä Confusion Matrix:\")\nprint(confusion_matrix(y_true, y_pred))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T15:50:33.199308Z","iopub.execute_input":"2025-05-31T15:50:33.199506Z","iopub.status.idle":"2025-05-31T15:50:36.440859Z","shell.execute_reply.started":"2025-05-31T15:50:33.199490Z","shell.execute_reply":"2025-05-31T15:50:36.440062Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, precision_recall_fscore_support\n\ndef plot_full_evaluation(model, dataloader, classes):\n    model.eval()\n    y_true = []\n    y_pred = []\n\n    with torch.no_grad():\n        for images, targets in dataloader:\n            images = [img.to(device) for img in images]\n            outputs = model(images)\n\n            for i in range(len(outputs)):\n                pred_labels = outputs[i]['labels'].cpu().numpy()\n                true_labels = targets[i]['labels'].cpu().numpy()\n\n                min_len = min(len(pred_labels), len(true_labels))\n                y_true.extend(true_labels[:min_len])\n                y_pred.extend(pred_labels[:min_len])\n\n    # === 1. T·ªïng h·ª£p Precision / Recall / F1 / Accuracy ===\n    print(\"üìä Precision / Recall / F1-score / Accuracy:\")\n    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro', zero_division=0)\n    acc = accuracy_score(y_true, y_pred)\n    print(f\"  ‚úÖ Accuracy:  {acc:.4f}\")\n    print(f\"  üéØ Precision: {precision:.4f}\")\n    print(f\"  üì• Recall:    {recall:.4f}\")\n    print(f\"  üí° F1-score:  {f1:.4f}\")\n\n    # === 2. Confusion Matrix ===\n    plt.figure(figsize=(6, 5))\n    ConfusionMatrixDisplay.from_predictions(\n        y_true, y_pred, display_labels=classes, cmap='Blues', values_format='d')\n    plt.title(\"Confusion Matrix\")\n    plt.grid(False)\n    plt.show()\n\n    # === 3. Bi·ªÉu ƒë·ªì theo t·ª´ng l·ªõp ===\n    pr, rc, f1_class, _ = precision_recall_fscore_support(y_true, y_pred, labels=range(len(classes)), zero_division=0)\n    x = range(len(classes))\n    plt.figure(figsize=(8, 4))\n    plt.bar(x, pr, width=0.2, label=\"Precision\", align='center')\n    plt.bar([i + 0.2 for i in x], rc, width=0.2, label=\"Recall\", align='center')\n    plt.bar([i + 0.4 for i in x], f1_class, width=0.2, label=\"F1-score\", align='center')\n    plt.xticks([i + 0.2 for i in x], classes)\n    plt.ylabel(\"Score\")\n    plt.title(\"Precision / Recall / F1 per Class\")\n    plt.legend()\n    plt.tight_layout()\n    plt.show()\n\n# ‚úÖ G·ªçi ƒë√°nh gi√° tr√™n t·∫≠p valid/test\nplot_full_evaluation(model, validloader, classes)  # ho·∫∑c testloader n·∫øu b·∫°n mu·ªën\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T16:04:10.030988Z","iopub.execute_input":"2025-05-31T16:04:10.031520Z","iopub.status.idle":"2025-05-31T16:04:13.718519Z","shell.execute_reply.started":"2025-05-31T16:04:10.031494Z","shell.execute_reply":"2025-05-31T16:04:13.717774Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\nimport matplotlib.patches as patches\nfrom PIL import Image\n\ndef visualize_prediction(model, dataset, class_names):\n    model.eval()\n\n    # L·∫•y ng·∫´u nhi√™n 1 ·∫£nh t·ª´ t·∫≠p test\n    idx = random.randint(0, len(dataset)-1)\n    image, _ = dataset[idx]  # l·∫•y ·∫£nh (kh√¥ng c·∫ßn ground truth)\n\n    # Chuy·ªÉn ·∫£nh sang thi·∫øt b·ªã v√† th√™m batch dim\n    input_tensor = image.to(device).unsqueeze(0)\n\n    with torch.no_grad():\n        outputs = model(input_tensor)\n\n    output = outputs[0]\n    boxes = output['boxes'].cpu().numpy()\n    labels = output['labels'].cpu().numpy()\n    scores = output['scores'].cpu().numpy()\n\n    # Hi·ªÉn th·ªã ·∫£nh + bbox\n    image_np = image.permute(1, 2, 0).cpu().numpy()\n\n    fig, ax = plt.subplots(1, figsize=(8, 8))\n    ax.imshow(image_np)\n    for box, label, score in zip(boxes, labels, scores):\n        if score < 0.5:\n            continue  # b·ªè qua k·∫øt qu·∫£ score th·∫•p\n        xmin, ymin, xmax, ymax = box\n        rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n                                 linewidth=2, edgecolor='lime', facecolor='none')\n        ax.add_patch(rect)\n        ax.text(xmin, ymin - 5, f\"{class_names[label]} ({score:.2f})\",\n                color='yellow', fontsize=10, weight='bold',\n                bbox=dict(facecolor='black', alpha=0.5, boxstyle='round,pad=0.2'))\n    plt.title(\"üì∑ D·ª± ƒëo√°n t·ª´ m√¥ h√¨nh Faster R-CNN\")\n    plt.axis('off')\n    plt.show()\n\n# ‚úÖ G·ªçi h√†m ƒë·ªÉ hi·ªÉn th·ªã ·∫£nh test b·∫•t k·ª≥\nvisualize_prediction(model, testset, classes)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T16:03:57.171898Z","iopub.execute_input":"2025-05-31T16:03:57.172215Z","iopub.status.idle":"2025-05-31T16:03:57.251961Z","shell.execute_reply.started":"2025-05-31T16:03:57.172192Z","shell.execute_reply":"2025-05-31T16:03:57.251019Z"}},"outputs":[],"execution_count":null}]}